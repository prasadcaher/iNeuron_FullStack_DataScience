{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2aa57f2",
   "metadata": {},
   "source": [
    "# Assignment 08"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d4f3bd",
   "metadata": {},
   "source": [
    "##### 1. What exactly is a feature? Give an example to illustrate your point ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797d1cdb",
   "metadata": {},
   "source": [
    "**Ans:** Features are the basic building blocks of datasets. The quality of the features in your dataset has a major impact on the quality of the insights you will gain when you use that dataset for machine learning. \n",
    "\n",
    "Additionally, different business problems within the same industry do not necessarily require the same features, which is why it is important to have a strong understanding of the business goals of your data science project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8337ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example we can take a look at the diabetes dataset loaded from sllearn library\n",
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a31f56cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df,target = load_diabetes(return_X_y=True, as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "984a2ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>disease_progression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>-0.017646</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068330</td>\n",
       "      <td>-0.092204</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>-0.009362</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>-0.046641</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>0.059744</td>\n",
       "      <td>-0.005697</td>\n",
       "      <td>-0.002566</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.031193</td>\n",
       "      <td>0.007207</td>\n",
       "      <td>178.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>-0.005515</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>-0.067642</td>\n",
       "      <td>0.049341</td>\n",
       "      <td>0.079165</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>-0.018118</td>\n",
       "      <td>0.044485</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>0.017282</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.013840</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>-0.011080</td>\n",
       "      <td>-0.046879</td>\n",
       "      <td>0.015491</td>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.016318</td>\n",
       "      <td>0.015283</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.026560</td>\n",
       "      <td>0.044528</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.073030</td>\n",
       "      <td>-0.081414</td>\n",
       "      <td>0.083740</td>\n",
       "      <td>0.027809</td>\n",
       "      <td>0.173816</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.004220</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0    0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1   -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2    0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
       "3   -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4    0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "437  0.041708  0.050680  0.019662  0.059744 -0.005697 -0.002566 -0.028674   \n",
       "438 -0.005515  0.050680 -0.015906 -0.067642  0.049341  0.079165 -0.028674   \n",
       "439  0.041708  0.050680 -0.015906  0.017282 -0.037344 -0.013840 -0.024993   \n",
       "440 -0.045472 -0.044642  0.039062  0.001215  0.016318  0.015283 -0.028674   \n",
       "441 -0.045472 -0.044642 -0.073030 -0.081414  0.083740  0.027809  0.173816   \n",
       "\n",
       "           s4        s5        s6  disease_progression  \n",
       "0   -0.002592  0.019908 -0.017646                151.0  \n",
       "1   -0.039493 -0.068330 -0.092204                 75.0  \n",
       "2   -0.002592  0.002864 -0.025930                141.0  \n",
       "3    0.034309  0.022692 -0.009362                206.0  \n",
       "4   -0.002592 -0.031991 -0.046641                135.0  \n",
       "..        ...       ...       ...                  ...  \n",
       "437 -0.002592  0.031193  0.007207                178.0  \n",
       "438  0.034309 -0.018118  0.044485                104.0  \n",
       "439 -0.011080 -0.046879  0.015491                132.0  \n",
       "440  0.026560  0.044528 -0.025930                220.0  \n",
       "441 -0.039493 -0.004220  0.003064                 57.0  \n",
       "\n",
       "[442 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['disease_progression'] = target\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc58a77",
   "metadata": {},
   "source": [
    "Here we have to predict **disease_progression** on the basis of given features like age, bmi, bp etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941bc9d9",
   "metadata": {},
   "source": [
    "##### 2. What are the various circumstances in which feature construction is required ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f0d0e7",
   "metadata": {},
   "source": [
    "**Ans:** The features in the data will directly influence the predictive models we use and the results we can achieve. Our results are dependent on many inter-dependent properties. We need great features that describe the structures inherent in the data. Better features means flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a53a7ce",
   "metadata": {},
   "source": [
    "##### 3. Describe how nominal variables are encoded ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3b1d24",
   "metadata": {},
   "source": [
    "**Ans:** \n",
    "Categorical variables are often called nominal.\n",
    "\n",
    "Some examples include:\n",
    "\n",
    "A “pet” variable with the values: “dog” and “cat“.\n",
    "A “color” variable with the values: “red“, “green“, and “blue“.\n",
    "A “place” variable with the values: “first“, “second“, and “third“.\n",
    "\n",
    "Nominal Variable (Categorical) - Variable comprises a finite set of discrete values with no relationship between values. —  mean and median are meaningless. Nominal data is classified without a natural order or rank\n",
    "\n",
    "**Encoding Categorical Data**\n",
    "There are three common approaches for converting ordinal and categorical variables to numerical values. They are:\n",
    "\n",
    "- Ordinal Encoding\n",
    "- One-Hot Encoding\n",
    "- Dummy Variable Encoding\n",
    "\n",
    "***Ordinal Encoding***\n",
    "In ordinal encoding, each unique category value is assigned an integer value.\n",
    "\n",
    "For example, “red” is 1, “green” is 2, and “blue” is 3.\n",
    "\n",
    "This is called an ordinal encoding or an integer encoding and is easily reversible. Often, integer values starting at zero are used.\n",
    "\n",
    "***One-Hot Encoding***\n",
    "For categorical variables where no ordinal relationship exists, the integer encoding may not be enough, at best, or misleading to the model at worst.\n",
    "\n",
    "Forcing an ordinal relationship via an ordinal encoding and allowing the model to assume a natural ordering between categories may result in poor performance or unexpected results (predictions halfway between categories).\n",
    "\n",
    "In this case, a one-hot encoding can be applied to the ordinal representation. This is where the integer encoded variable is removed and one new binary variable is added for each unique integer value in the variable.\n",
    "\n",
    "In the “color” variable example, there are three categories, and, therefore, three binary variables are needed. A “1” value is placed in the binary variable for the color and “0” values for the other colors.\n",
    "\n",
    "***Dummy Variable Encoding***\n",
    "The one-hot encoding creates one binary variable for each category.\n",
    "\n",
    "The problem is that this representation includes redundancy. For example, if we know that [1, 0, 0] represents “blue” and [0, 1, 0] represents “green” we don’t need another binary variable to represent “red“, instead we could use 0 values for both “blue” and “green” alone, e.g. [0, 0].\n",
    "\n",
    "This is called a dummy variable encoding, and always represents C categories with C-1 binary variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4de4555",
   "metadata": {},
   "source": [
    "##### 4. Describe how numeric features are converted to categorical features ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2ce760",
   "metadata": {},
   "source": [
    "**Ans:** Techniques to Encode Numerical features to Categorical Features\n",
    "\n",
    "***Discretization***: It is the process of transforming continuous variables into categorical variables by creating a set of intervals, which are contiguous, that span over the range of the variable’s values. It is also known as “Binning”, where the bin is an analogous name for an interval.\n",
    "\n",
    "Benefits of Discretization:\n",
    "\n",
    "1. Handles the Outliers in a better way.\n",
    "\n",
    "2. Improves the value spread.\n",
    "\n",
    "3. Minimize the effects of small observation errors.\n",
    "\n",
    "\n",
    "***Types of Binning***:\n",
    "\n",
    " Unsupervised Binning:\n",
    " \n",
    "(a) Equal width binning:\n",
    "\n",
    "It is also known as “Uniform Binning” since the width of all the intervals is the same. The algorithm divides the data into N intervals of equal size. The width of intervals is:\n",
    "\n",
    "w=(max-min)/N\n",
    "\n",
    "Therefore, the interval boundaries are:\n",
    "[min+w], [min+2w], [min+3w], – – – – – – – – – – – -, [min+(N-1)w] where, min and max are the minimum and maximum value from the data respectively.\n",
    "This technique does not changes the spread of the data but does handle the outliers.\n",
    " \n",
    "\n",
    "(b) Equal frequency binning:\n",
    "\n",
    "It is also known as “Quantile Binning”. The algorithm divides the data into N groups where each group contains approximately the same number of values.\n",
    "\n",
    "Consider, we want 10 bins, that is each interval contains 10% of the total observations.\n",
    "Here the width of the interval need not necessarily be equal.\n",
    "Handles outliers better than the previous method and makes the value spread approximately uniform(each interval contains almost the same number of values).\n",
    "\n",
    " \n",
    "\n",
    "(c) K-means binning:\n",
    "\n",
    "This technique uses the clustering algorithm namely ” K-Means Algorithm”.\n",
    "\n",
    "This technique is mostly used when our data is in the form of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6109820",
   "metadata": {},
   "source": [
    "##### 5. Describe the feature selection wrapper approach. State the advantages and disadvantages of this approach ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72de65b0",
   "metadata": {},
   "source": [
    "**Ans:** Wrapper methods measure the “usefulness” of features based on the classifier performance. In contrast, the filter methods pick up the intrinsic properties of the features (i.e., the “relevance” of the features) measured via univariate statistics instead of cross-validation performance.\n",
    "\n",
    "The wrapper classification algorithms with joint dimensionality reduction and classification can also be used but these methods have high computation cost, lower discriminative power. Moreover, these methods depend on the efficient selection of classifiers for obtaining high accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51584401",
   "metadata": {},
   "source": [
    "##### 6. When is a feature considered irrelevant? What can be said to quantify it ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a92af6",
   "metadata": {},
   "source": [
    "**Ans:** Features are considered relevant if they are either strongly or weakly relevant, and are considered irrelevant otherwise. \n",
    "\n",
    "Irrelevant features can never contribute to prediction accuracy, by definition. Also to quantify it we need to first check the list of features, There are three types of feature selection:\n",
    "\n",
    "- **Wrapper methods** (forward, backward, and stepwise selection)\n",
    "- **Filter methods** (ANOVA, Pearson correlation, variance thresholding)\n",
    "- **Embedded methods** (Lasso, Ridge, Decision Tree)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d21fc3f",
   "metadata": {},
   "source": [
    "##### 7. When is a function considered redundant? What criteria are used to identify features that could be redundant ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0ce42e",
   "metadata": {},
   "source": [
    "**Ans:** If two features `{X1, X2}` are highly correlated, then the two features become redundant features since they have same information in terms of correlation measure. In other words, the correlation measure provides statistical association between any given a pair of features. \n",
    "\n",
    "Minimum redundancy feature selection is an algorithm frequently used in a method to accurately identify characteristics of genes and phenotypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd4a123",
   "metadata": {},
   "source": [
    "##### 8. What are the various distance measurements used to determine feature similarity ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed81d5c4",
   "metadata": {},
   "source": [
    "**Ans:** Four of the most commonly used distance measures in machine learning are as follows: \n",
    "- Hamming Distance. \n",
    "- Euclidean Distance\n",
    "- Manhattan Distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d6004e",
   "metadata": {},
   "source": [
    "##### 9. State difference between Euclidean and Manhattan distances ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39944ba",
   "metadata": {},
   "source": [
    "**Ans:** Euclidean & Hamming distances are used to measure similarity or dissimilarity between two sequences. Euclidean distance is extensively applied in analysis of convolutional codes and Trellis codes.\n",
    "\n",
    "Hamming distance is frequently encountered in the analysis of block codes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a5cfd2",
   "metadata": {},
   "source": [
    "##### 10. Distinguish between feature transformation and feature selection ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722dfcff",
   "metadata": {},
   "source": [
    "**Ans:** Feature selection is for filtering irrelevant or redundant features from your dataset. The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
