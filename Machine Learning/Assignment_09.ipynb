{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2aa57f2",
   "metadata": {},
   "source": [
    "# Assignment 09"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111120c6",
   "metadata": {},
   "source": [
    "##### 1. What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0d4a0d",
   "metadata": {},
   "source": [
    "**Ans:** Feature engineering is the process of selecting, manipulating, and transforming raw data into features that can be used in supervised learning. In order to make machine learning work well on new tasks, it might be necessary to design  and train better features.\n",
    "      \n",
    "Feature engineering in ML consists of four main steps: Feature Creation, Transformations, Feature Extraction, and  Feature Selection. Feature engineering consists of creation, transformation, extraction, and selection of features, also known as variables, that are most conducive to creating an accurate ML algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae857d4",
   "metadata": {},
   "source": [
    "##### 2. What is feature selection, and how does it work? What is the aim of it? What are the various methods of function selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2906477",
   "metadata": {},
   "source": [
    "**Ans:** Feature Selection is the process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in. Having irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevant features. \n",
    "\n",
    "There are three types of feature selection:\n",
    "- **Wrapper methods** (forward, backward, and stepwise selection)\n",
    "- **Filter methods** (ANOVA, Pearson correlation, variance thresholding)\n",
    "- **Embedded methods** (Lasso, Ridge, Decision Tree)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c981ac1e",
   "metadata": {},
   "source": [
    "##### 3. Describe the function selection filter and wrapper approaches. State the pros and cons of each approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70c861e",
   "metadata": {},
   "source": [
    "**Ans:** The main differences between the filter and wrapper methods for feature selection are: Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a  subset of feature by actually training a model on it.\n",
    "      \n",
    "The filter method has the fastest running time; however, it does not consider feature dependencies and tends to each feature separately when univariate techniques are used. The wrapper method has the advantages of better  generalization and robust interaction with the classifier used for feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab2a64",
   "metadata": {},
   "source": [
    "##### 4. Please Answer the following Questions :\n",
    "1. Describe the overall feature selection process.\n",
    "2. Explain the key underlying principle of feature extraction using an example. What are the most widely used feature extraction algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812b9aad",
   "metadata": {},
   "source": [
    "**Ans:** \n",
    "\n",
    "1.\n",
    "\n",
    "***Feature selection*** is the process of reducing the number of input variables when developing a predictive model.\n",
    "\n",
    "It is desirable to reduce the number of input variables to both reduce the computational ***cost of modeling*** and, in some cases, to improve the ***performance of the model***.\n",
    "\n",
    "*Statistical-based* feature selection methods involve evaluating the relationship between each input variable and the target variable using statistics and selecting those input variables that have the strongest relationship with the target variable.\n",
    "\n",
    "These methods can be fast and effective, although the choice of statistical measures depends on the data type of both the input and output variables.\n",
    "\n",
    "**Feature Selection Methods**\n",
    "\n",
    "Feature selection methods are intended to reduce the number of input variables to those that are believed to be most useful to a model in order to predict the target variable.\n",
    "\n",
    "`Feature selection is primarily focused on removing non-informative or redundant predictors from the model.`\n",
    "\n",
    "*Filter Methods*\n",
    "\n",
    "Filter feature selection methods apply a statistical measure to assign a scoring to each feature. The features are ranked by the score and either selected to be kept or removed from the dataset. The methods are often univariate and consider the feature independently, or with regard to the dependent variable.\n",
    "\n",
    "Some examples of some filter methods include the Chi squared test, information gain and correlation coefficient scores.\n",
    "\n",
    "*Wrapper Methods*\n",
    "\n",
    "Wrapper methods consider the selection of a set of features as a search problem, where different combinations are prepared, evaluated and compared to other combinations. A predictive model is used to evaluate a combination of features and assign a score based on model accuracy.\n",
    "\n",
    "The search process may be methodical such as a best-first search, it may stochastic such as a random hill-climbing algorithm, or it may use heuristics, like forward and backward passes to add and remove features.\n",
    "\n",
    "An example if a wrapper method is the recursive feature elimination algorithm.\n",
    "\n",
    "*Embedded Methods*\n",
    "\n",
    "Embedded methods learn which features best contribute to the accuracy of the model while the model is being created. The most common type of embedded feature selection methods are regularization methods.\n",
    "\n",
    "Regularization methods are also called penalization methods that introduce additional constraints into the optimization of a predictive algorithm (such as a regression algorithm) that bias the model toward lower complexity (fewer coefficients).\n",
    "\n",
    "Examples of regularization algorithms are the LASSO, Elastic Net and Ridge Regression.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbeab2b",
   "metadata": {},
   "source": [
    "2.\n",
    "\n",
    "Feature extraction is a part of the dimensionality reduction process, in which, an initial set of the raw data is divided and reduced to more manageable groups. So when you want to process it will be easier. \n",
    "\n",
    "The most important characteristic of these large data sets is that they have a large number of variables. These variables require a lot of computing resources to process. So Feature extraction helps to get the best feature from those big data sets by selecting and combining variables into features, thus, effectively reducing the amount of data. \n",
    "\n",
    "These features are easy to process, but still able to describe the actual data set with accuracy and originality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef273e2",
   "metadata": {},
   "source": [
    " Example   \n",
    " PCA is a method of obtaining important variables (in form of components) from a large set of variables available in a data set. It tends to find the direction of maximum variation (spread) in data. PCA is more useful when dealing with 3 or higher-dimensional data.\n",
    "\n",
    "PCA can be used for anomaly detection and outlier detection because they will not be part of the data as it would be considered noise by PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2162e",
   "metadata": {},
   "source": [
    "##### 5. Describe the feature engineering process in the sense of a text categorization issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99edb141",
   "metadata": {},
   "source": [
    "**Ans:** Text classification is the problem of assigning categories to text data according to its content. The most important part of text classification is feature engineering: the process of creating features for a machine learning model from  raw text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96163dd",
   "metadata": {},
   "source": [
    "##### 6. What makes cosine similarity a good metric for text categorization? A document-term matrix has two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in cosine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4999ef6",
   "metadata": {},
   "source": [
    "**Ans:** Cosine similarity is a metric used to measure how similar the documents are irrespective of their size. The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together.\n",
    "\n",
    "Cosine similarity is the cosine of the angle between two n-dimensional vectors in an n-dimensional space. It is the dot product of the two vectors divided by the product of the two vectors' lengths (or magnitudes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e636eaf",
   "metadata": {},
   "source": [
    "##### 7. Explain the following:\n",
    "1. What is the formula for calculating Hamming distance? Between 10001011 and 11001111, calculate the Hamming gap.\n",
    "2. Compare the Jaccard index and similarity matching coefficient of two features with values `(1,1,0,0,1,0,1,1)` and `(1,1,0,0, 0,1,1,1)`, respectively `(1,0,0,1,1,0,0,1)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3ea32c",
   "metadata": {},
   "source": [
    "**Ans:** Thus the Hamming distance between two vectors is the number of bits we must change to change one into the other. Example Find the distance between the vectors `01101010` and `11011011`. They differ in four places, so the Hamming distance `d(01101010,11011011) = 4`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64feb7de",
   "metadata": {},
   "source": [
    "##### 8. State what is meant by  \"high-dimensional data set\"? Could you offer a few real-life examples? What are the difficulties in using machine learning techniques on a data set with many dimensions? What can be done about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da31e0f",
   "metadata": {},
   "source": [
    "**Ans:** High dimension is when variable numbers p is higher than the sample sizes n i.e. p>n, cases. High dimensional data is referred to a data of n samples with p features, where p is larger than n.\n",
    "\n",
    "For example, tomographic imaging data, ECG data, and MEG data. One example of high dimensional data is microarray gene expression data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec9c51",
   "metadata": {},
   "source": [
    "##### 9. Make a few quick notes on:\n",
    "1. PCA is an acronym for Personal Computer Analysis.\n",
    "2. Use of vectors\n",
    "3. Embedded technique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9821bb5b",
   "metadata": {},
   "source": [
    "**Ans:** The Principal component analysis (PCA) is a technique used for identification of a smaller number of uncorrelated variables known as principal components from a larger set of data. The technique is widely used to emphasize variation  and capture strong patterns in a data set.\n",
    "\n",
    "Vectors can be used to represent physical quantities. Most commonly in physics, vectors are used to represent displacement, velocity, and acceleration. Vectors are a combination of magnitude and direction, and are drawn as arrows\n",
    "\n",
    "In the context of machine learning, an embedding is a low-dimensional, learned continuous vector representation of discrete variables into which you can translate high-dimensional vectors. Generally, embeddings make ML models more efficient and easier to work with, and can be used with other models as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5830b034",
   "metadata": {},
   "source": [
    "##### 10. Make a comparison between:\n",
    "1. Sequential backward exclusion vs. sequential forward selection\n",
    "2. Function selection methods: filter vs. wrapper\n",
    "3. SMC vs. Jaccard coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994a8c23",
   "metadata": {},
   "source": [
    "**Ans:** Sequential floating forward selection (SFFS) starts from the empty set. After each forward step, SFFS performs backward steps as long as the objective function increases. Sequential floating backward selection (SFBS) starts from the full set.\n",
    "\n",
    "The Jaccard coefficient is a measure of the percentage of overlap between sets defined as: (5.1) where W1 and W2 are two sets, in our case the 1-year windows of the ego networks. The Jaccard coefficient can be a value between 0 and 1, with 0 indicating no overlap and 1 complete overlap between the sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e55792a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruit = {\"apple\", \"banana\", \"orange\", \"grapefruit\"}\n",
    "type(fruit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
